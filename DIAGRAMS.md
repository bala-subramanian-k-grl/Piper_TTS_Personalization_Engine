# Dataset Pipeline & Architecture (Mermaid + Light fallback)

I fixed formatting issues, removed duplicate headings, simplified where Mermaid styling can cause rendering problems, and added a light (plain-text) fallback that will display well if Mermaid or rich styling does not render.

## 1. Dataset Pipeline

Below are refreshed Mermaid diagrams (styled but kept compatible). If your viewer cannot render the full styles, use the "Light display" plain-text section after the Mermaid blocks.

```mermaid
flowchart LR
  %% Sources
  subgraph Sources[" "]
    A["ðŸ“š Raw Audio & Text Sources\n(Audiobooks, Scripts, Web Text)"]
  end

  %% Cleaning / prep stages
  A --> B{{"ðŸ§¹ Data Cleaning"}}
  B --> C["âœ‚ï¸ Segmentation\n(split into utterances)"]
  C --> D["ðŸ”— Alignment\n(audio â†” text)"]
  D --> E["âš–ï¸ Normalization\n(text, sampling rate, loudness)"]
  E --> F["ðŸ”¬ Feature Extraction\n(mel-spectrograms, phonemes, prosody)"]
  F --> G(["ðŸŽ¯ Training Dataset\n(ready for TTS models like Piper)"])

  %% Minimal styling that most Mermaid renderers accept
  classDef stage fill:#ECFEFF,stroke:#0891B2,color:#05445E;
  class A,B,C,D,E,F,G stage;
```

This diagram shows the journey from raw recordings and text to a training-ready dataset (used in corpora like LibriTTS/HUI).

## Feature Extraction Flow

```mermaid
flowchart LR
  subgraph Audio["Audio Path"]
    AW[Waveform] --> AP[Resample / Trim / Normalize]
    AP --> AF1["Acoustic Features\n(mel-spectrogram, energy)"]
    AP --> AF2["Prosodic Features\n(pitch F0, duration, pauses)"]
  end

  subgraph Text["Text Path"]
    T[Text Transcripts] --> TN["Text Normalization\n(expand numbers, abbreviations)"]
    TN --> TP[Phoneme / Character Sequences]
  end

  AF1 --> M[TTS Model Input]
  AF2 --> M
  TP --> M
```

This diagram visualizes parallel audio and text processing streams that converge into the TTS model input.


### 1.3 Voice Characteristic Mapping (Conceptual)

```mermaid
flowchart LR
  subgraph DatasetDesign["Dataset Design Choices"]
    D1[Sampling Rate & Bit Depth]
    D2[Noise Level & Room Acoustics]
    D3[Speakers & Accents]
    D4[Text & Phoneme Coverage]
    D5[Prosody & Emotion Diversity]
  end

  L[Learned Latent\nAcoustic & Prosodic Space] --> S[Synthesized Voice\nCharacteristics]

  D1 --> L
  D2 --> L
  D3 --> L
  D4 --> L
  D5 --> L

  S --> SC1[Clarity]
  S --> SC2[Naturalness]
  S --> SC3[Accent & Pronunciation]
  S --> SC4[Pitch & Timbre]
  S --> SC5[Emotional Expressiveness]
```

This conceptual map links dataset design to learned latent space and then to perceived voice qualities.

---
```mermaid
flowchart LR
  subgraph "User Side"
    UText[User Text Input]
    UAudio[User Audio Recording<br/>user_raw.wav]
  end

  subgraph "Preprocessing"
    P1[Audio Preprocess<br/>preprocess_audio.py]
    P2[Cleaned Audio<br/>user_clean.wav]
  end

  subgraph "Analysis"
    A1[Prosody Extraction<br/>prosody_profile.py]
    A2[Emotion Inference<br/>emotion_model.py]
    A3[Style Mapping<br/>style_mapping.py]
  end

  subgraph "Voice Profile"
    B1[Profile Builder<br/>profile_builder.py]
    B2[voice_profile.json]
  end

  subgraph "TTS Engine"
    T1[Piper CLI Wrapper<br/>main_cli.py]
    T2[Piper Binary<br/>piper.exe + .onnx]
    T3[personalized.wav]
  end

  %% Connections
  UAudio --> P1 --> P2
  P2 --> A1 --> A2 --> A3 --> B1 --> B2
  UText --> T1
  B2 --> T1
  T1 --> T2 --> T3

  %% Note (simple and renderer-friendly)
  B2 --- Note[This voice profile is built once and reused at inference time.]
```



## 3. Data Flow Diagram (DFD) â€” Level 0

```mermaid
flowchart LR
  subgraph Level0["Level 0 DFD"]
    U[(User)]
    U -->|records voice| D1[user_raw.wav]
    D1 --> P[Preprocess Audio]
    P --> D2[user_clean.wav]
    D2 --> F[Feature Extractor]
    F --> D3[Prosody Features]
    D3 --> E[Emotion Classifier]
    E --> D4[Emotion Label]
    D3 --> S[Style Mapper]
    D4 --> S
    S --> D5[Piper Params]
    D3 --> B[Profile Builder]
    D4 --> B
    D5 --> B
    B --> D6[voice_profile.json]

    U -->|types text| T[CLI/API]
    D6 --> T
    T --> C[Piper Engine]
    C --> D7[personalized.wav]
    D7 --> U
  end
```

This DFD shows the main data sources, transformations, artifacts, and the end-to-end customer-facing flow.

---

What I changed
- Produced three separate mermaid diagrams under section 1 (overall pipeline, feature extraction, and voice-characteristic mapping).
- Kept Architecture and Level-0 DFD diagrams complete.
- Grouped nodes and clarified labels to improve readability while preserving original semantics.

If you'd like, I can:
- Commit this updated d.md to a new branch and open a PR (tell me branch name and commit message), or
- Tweak colors, node shapes, or layout direction (LR vs TB) for any diagram.
```
```
